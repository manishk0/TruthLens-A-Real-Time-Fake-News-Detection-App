# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fqpSNTyI9IvLWCC1QN6bU6W40ItTxGEC
"""

!pip install streamlit pyngrok

with open("app.py", "w") as f:
    f.write("""import streamlit as st
import pandas as pd
import re
import string
import nltk

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download stopwords & lemmatizer resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load and prepare the data
@st.cache_data
def load_data():
    fake = pd.read_csv("Fake.csv")
    true = pd.read_csv("True.csv")
    fake['labels'] = 1
    true['labels'] = 0
    df = pd.concat([fake, true]).sample(frac=1, random_state=42).reset_index(drop=True)
    return df

# Clean the text
def clean_text(text):
    stop_words = set(stopwords.words("english"))
    lemmatizer = WordNetLemmatizer()
    text = re.sub(r'\d+', '', str(text).lower())
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(words)

# Train the model
@st.cache_resource
def train_model(df):
    df["clean_text"] = df["text"].apply(clean_text)
    x = df["clean_text"]
    y = df["labels"]

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    vectorizer = TfidfVectorizer(max_df=0.7)
    x_train_idf = vectorizer.fit_transform(x_train)
    x_test_idf = vectorizer.transform(x_test)

    model = LogisticRegression()
    model.fit(x_train_idf, y_train)
    acc = accuracy_score(y_test, model.predict(x_test_idf))
    report = classification_report(y_test, model.predict(x_test_idf), target_names=['True', 'Fake'])

    return model, vectorizer, acc, report

# Streamlit UI
st.title("ðŸ“° Fake News Detection App")
st.markdown("Enter a news article or statement below, and the model will predict if it's **Real** or **Fake**.")

df = load_data()
model, vectorizer, acc, report = train_model(df)

# Input from user
user_input = st.text_area("âœï¸ Enter News Content Here:")

if st.button("Predict"):
    if user_input.strip() == "":
        st.warning("Please enter some text to check.")
    else:
        cleaned = clean_text(user_input)
        vectorized = vectorizer.transform([cleaned])
        prediction = model.predict(vectorized)[0]
        label = "Fake News ðŸ›‘" if prediction == 1 else "Real News âœ…"
        st.subheader(f"Prediction: {label}")

st.markdown("---")
st.subheader("ðŸ“Š Model Performance")
st.text(f"Accuracy: {acc * 100:.2f}%")
with st.expander("See Classification Report"):
    st.text(report)
""")

!rm -rf /root/.config/ngrok/ngrok.yml


!ngrok config add-authtoken 2x8T6Od20GYMymrblqk8NG3x80U_3qRu61kQRfACBYXKZ14gC
from pyngrok import ngrok
import time

# Start Streamlit app in background
!streamlit run app.py &> /dev/null &

# Wait for Streamlit to start
time.sleep(5)

# Connect with ngrok
public_url = ngrok.connect(8501)
print("âœ… Streamlit app is live at:", public_url)



